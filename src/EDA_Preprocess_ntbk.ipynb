{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b89a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1caff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 20}\n",
    "\n",
    "mpl.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "quake_data_df = pd.read_csv('Data/blended_quake_data.csv')\n",
    "quake_data_df = quake_data_df[(quake_data_df['DEATHS'] < 100000) & (quake_data_df['EQ_PRIMARY'] > 3)]\n",
    "quake_data_df['Deaths_Logged'] = np.log10(quake_data_df['DEATHS'] + 1)\n",
    "quake_data_df['Pct_Pop_Fatalities'] = quake_data_df['DEATHS'] / quake_data_df['sum_density']\n",
    "\n",
    "# Create our target variable\n",
    "quake_data_df['Deaths_Logged_Category'] = quake_data_df['Deaths_Logged'].apply(lambda x: np.ceil(x))\n",
    "\n",
    "# landslide_vars = quake_data_df.iloc[:, 16:25].columns\n",
    "# print(landslide_vars)\n",
    "# landslide_df = pd.melt(quake_data_df, \n",
    "#         id_vars=[col for col in quake_data_df if col not in landslide_vars], \n",
    "#         value_vars=landslide_vars,\n",
    "#         var_name='Landslide_Cat').reset_index()\n",
    "quake_data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# landslide_df['Deaths_Logged'] = landslide_df['Deaths_Logged'].apply(lambda x: np.ceil(x))\n",
    "# landslide_df = landslide_df[landslide_df['value'] > 0]\n",
    "# landslide_df['Deaths_Logged'] = np.log10(landslide_df['DEATHS'] + 1)\n",
    "# ldsl_max_cat = landslide_df.groupby([col for col in quake_data_df if col not in landslide_vars]).agg(largest_cat=('Landslide_Cat', 'max')).reset_index()\n",
    "# ldsl_max_cat['HasDeath'] = np.where(ldsl_max_cat['DEATHS'] > 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519ab664",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logged Death Toll Hist (Unrounded)\n",
    "quake_data_df['Deaths_Logged'].hist(bins=25, figsize= (15, 12))\n",
    "\n",
    "plt.ylabel('Number of EarthQuakes [Count]')\n",
    "plt.xlabel('Death Toll Magnitude [log10(Deaths)]')\n",
    "plt.title('Distribution of Earthquakes by their Death Tolls: 1980 - 2020')\n",
    "\n",
    "# plt.savefig('Figures\\Quake_Death_Hist_cont')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c730eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logged Death Toll Hist (0 Deaths mapped to zero, 1-10 -> 1, 11-100 -> 2, etc)\n",
    "\n",
    "quake_data_df.groupby(['Deaths_Logged_Category']).agg({'DEATHS': 'count'}).rename({'DEATHS': 'Count'}, axis=1).reset_index() \\\n",
    "    .plot.bar(x='Deaths_Logged_Category', y='Count', figsize= (14, 14), rot=0)\n",
    "\n",
    "plt.ylabel('Number of EarthQuakes [Count]')\n",
    "plt.xlabel('Death Toll Magnitude [Ceil(log10(Deaths))]')\n",
    "plt.title('Distribution of Earthquakes by their Death Tolls: 1980 - 2020')\n",
    "\n",
    "for cat in range(quake_data_df['Deaths_Logged_Category'].nunique()):\n",
    "    num = quake_data_df[quake_data_df['Deaths_Logged_Category'] == cat].shape[0]\n",
    "    plt.text(cat - 0.25, num + 10, num)\n",
    "# plt.savefig('Figures\\Quake_Death_Hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b44d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude boxplots by death toll magnitude\n",
    "\n",
    "quake_data_df[['Deaths_Logged_Category', 'EQ_PRIMARY']].boxplot(by='Deaths_Logged_Category', figsize=(12, 10),\n",
    "                                                               medianprops = dict(linewidth=4.0, color='black'))\n",
    "plt.xlabel('Magnitude of Death Toll [Ceil(log10(Deaths))]')\n",
    "plt.ylabel('Earthquake Magnitude [Richter Scale]')\n",
    "plt.title('Earthquake Magnitude Distribution by Death Toll Magnitude')\n",
    "# plt.savefig('Figures\\Mag_Avg_By_Death_Scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDI boxplots by death toll magnitude\n",
    "\n",
    "quake_data_df[['Deaths_Logged_Category', 'HDI']].boxplot(by='Deaths_Logged_Category', figsize=(12, 10),\n",
    "                                                               medianprops = dict(linewidth=4.0, color='black'))\n",
    "plt.xlabel('Magnitude of Death Toll [Ceil(log10(Deaths))]')\n",
    "plt.ylabel('HDI')\n",
    "plt.title('HDI Distribution by Death Toll Magnitude')\n",
    "# plt.savefig('Figures\\HDI_Avg_By_Death_Scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085bde5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatterplots of Deaths and Magnitude with Population and HDI Colors \n",
    "\n",
    "cmap = plt.cm.rainbow\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=np.log10(quake_data_df.sum_density.min()), vmax=np.log10(quake_data_df.sum_density.max()))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "ax.scatter(quake_data_df['EQ_PRIMARY'], \n",
    "            quake_data_df['Deaths_Logged'],\n",
    "            color=cmap(norm(np.log10(quake_data_df.sum_density.values))), s=60)\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "fig.colorbar(sm, label='Population Density [log10 Quantity]')\n",
    "plt.title('Magnitude vs Deaths by Population Density')\n",
    "plt.ylabel('Death Toll [log10 Quantity]')\n",
    "plt.xlabel('Magnitude [Richter Scale]')\n",
    "# plt.savefig('Figures\\Mag_Vs_Deaths_wrt_Pop')\n",
    "plt.show()\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=quake_data_df.HDI.min(), vmax=quake_data_df.HDI.max())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "ax.scatter(quake_data_df['EQ_PRIMARY'], \n",
    "            quake_data_df['Deaths_Logged'],\n",
    "            color=cmap(norm(quake_data_df.HDI.values)))\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "fig.colorbar(sm, label='Local HDI')\n",
    "plt.title('Magnitude vs Deaths by HDI')\n",
    "plt.ylabel('Death Toll [log10 Quantity]')\n",
    "plt.xlabel('Magnitude [Richter Scale]')\n",
    "# plt.savefig('Figures\\Mag_Vs_Deaths_wrt_hdi')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586544b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another view of the population-deaths-magnitude scatter plot. Take the median population of all quakes that have the \n",
    "# same magnitude and death toll (ie, all points stacked on each other)\n",
    "\n",
    "by_death_mag_df = quake_data_df.groupby(['EQ_PRIMARY', 'Deaths_Logged']).agg(med_pop=('sum_density', 'median')).reset_index()\n",
    "cmap = plt.cm.rainbow\n",
    "norm = mpl.colors.Normalize(vmin=np.log10(by_death_mag_df.med_pop.min()), vmax=np.log10(by_death_mag_df.med_pop.max()))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "ax.scatter(by_death_mag_df['EQ_PRIMARY'], \n",
    "            by_death_mag_df['Deaths_Logged'],\n",
    "            color=cmap(norm(np.log10(by_death_mag_df.med_pop.values))), s=60)\n",
    "\n",
    "plt.title('Magnitude vs Deaths by Population Density')\n",
    "plt.ylabel('Death Toll [log10 Quantity]')\n",
    "plt.xlabel('Magnitude [Richter Scale]')\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "fig.colorbar(sm, label='Median Logged Population')\n",
    "# plt.savefig('Figures\\Mag_Vs_Deaths_wrt_pop_no_point_stack')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105501dd",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable is continuous, so we use f_regression and mutual_info_regression\n",
    "\n",
    "top_n = 10\n",
    "feature_cols = quake_data_df.copy()\n",
    "feature_cols.drop(['Deaths_Logged_Category', 'Pct_Pop_Fatalities', 'LATITUDE', 'LONGITUDE', \n",
    "                   'INTENSITY', 'FLAG_TSUNAMI', 'COUNTRY', 'Unnamed: 0', 'I_D',\n",
    "                   'DEATHS', 'Deaths_Logged', 'REGION_CODE'], axis=1, inplace=True)\n",
    "target = quake_data_df['Deaths_Logged']\n",
    "\n",
    "\n",
    "# Linear correlations\n",
    "\n",
    "# Calculate f-score\n",
    "f_scores, p_values = f_regression(X=feature_cols, y=target)\n",
    "\n",
    "# Find 5 most important features\n",
    "lin_f_selector = SelectKBest(f_regression, k=top_n)\n",
    "best_lin_fs = lin_f_selector.fit_transform(X=feature_cols, y=target)\n",
    "lin_feature_mask = lin_f_selector.get_support()\n",
    "lin_feature_labels = lin_f_selector.get_feature_names_out()\n",
    "lin_feature_data = pd.Series(f_scores[lin_feature_mask], index=lin_feature_labels).sort_values(ascending=False)\n",
    "\n",
    "# Plot top 5 features\n",
    "plt.barh(y=lin_feature_data.index, width=lin_feature_data)\n",
    "plt.title(f'Top {top_n} Most Important Features Based on Linear Correlation')\n",
    "plt.xlabel('F-Statistic')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Non-linear correlations\n",
    "\n",
    "# Calculate mutual information\n",
    "mi = mutual_info_regression(X=feature_cols, y=target, random_state=112)\n",
    "\n",
    "# Find 5 most important features\n",
    "nonlin_f_selector = SelectKBest(mutual_info_regression, k=top_n)\n",
    "best_nonlin_fs = nonlin_f_selector.fit_transform(X=feature_cols, y=target)\n",
    "nonlin_feature_mask = nonlin_f_selector.get_support()\n",
    "nonlin_feature_labels = nonlin_f_selector.get_feature_names_out()\n",
    "nonlin_feature_data = pd.Series(mi[nonlin_feature_mask], index=nonlin_feature_labels).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Plot top n features\n",
    "plt.barh(y=nonlin_feature_data.index, width=nonlin_feature_data)\n",
    "plt.title(f'Top {top_n} Most Important Features Based on NonLinear Correlation')\n",
    "plt.xlabel('Mutual Information Measure')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c33132",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = quake_data_df.copy()\n",
    "temp_df.drop(['LandUseCode', 'I_D', 'Unnamed: 0', 'REGION_CODE', 'MONTH', 'dist_to_land_code',\n",
    "             'LATITUDE', 'LONGITUDE', 'INTENSITY', 'YEAR'], axis=1, inplace=True)\n",
    "cor_type = 'spearman'\n",
    "correl_matrix = temp_df.corr(method=cor_type)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(25, 25)\n",
    "plt.pcolormesh(correl_matrix, vmin=-1, vmax=1, cmap='seismic')\n",
    "plt.yticks(np.arange(0.5, len(correl_matrix.index), 1), correl_matrix.index)\n",
    "plt.xticks(np.arange(0.5, len(correl_matrix.columns), 1), correl_matrix.columns, rotation='vertical')\n",
    "\n",
    "# Credit to stackoverflow for this method of adding text captions: \n",
    "# https://stackoverflow.com/questions/11917547/how-to-annotate-heatmap-with-text-in-matplotlib\n",
    "for y in range(correl_matrix.shape[0]):\n",
    "    for x in range(correl_matrix.shape[1]):\n",
    "        if correl_matrix.iloc[y, x] < -0.35 or correl_matrix.iloc[y, x] >= 0.89:\n",
    "            plt.text(x + 0.5, y + 0.5, '%.2f' % correl_matrix.iloc[y, x],\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center', c='white')\n",
    "        else:\n",
    "            plt.text(x + 0.5, y + 0.5, '%.2f' % correl_matrix.iloc[y, x],\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center')\n",
    "\n",
    "plt.colorbar(label=f'{cor_type} Correlation Coefficient')\n",
    "plt.title('Correlation Matrix of All Features in Quake Dataset')\n",
    "# plt.savefig(f'Figures\\Feature_Correlation_Matrix_{cor_type}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10db9ca",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_split(X, y, train_size, val_size, test_size, random_state, stratify: bool = False):\n",
    "    \n",
    "    # test the inputs\n",
    "    if sum([train_size, val_size, test_size]) != 1:\n",
    "        raise ValueError('Train, test, and validation proportions do not equal 1!')\n",
    "    \n",
    "    if type(random_state) != int:\n",
    "        raise ValueError('random_state is not an integer!')\n",
    "        \n",
    "    if not stratify:\n",
    "    # perform basic split\n",
    "        X_train, X_other, y_train, y_other = train_test_split(X, y, train_size=train_size, random_state=random_state)\n",
    "\n",
    "        rem_size = 1 - train_size\n",
    "        new_test_size = test_size / rem_size\n",
    "\n",
    "        X_test, X_val, y_test, y_val = train_test_split(X_other, y_other, train_size=new_test_size, random_state=random_state)\n",
    "    \n",
    "    else:\n",
    "        # Stratify Split\n",
    "        X_train, X_other, y_train, y_other = train_test_split(X, y, train_size=train_size, \n",
    "                                                              random_state=random_state,\n",
    "                                                              stratify=y)\n",
    "\n",
    "        rem_size = 1 - train_size\n",
    "        new_test_size = test_size / rem_size\n",
    "\n",
    "        X_test, X_val, y_test, y_val = train_test_split(X_other, y_other, train_size=new_test_size, \n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=y_other)\n",
    "        \n",
    "    # test the outputs\n",
    "    if np.around(X_train.shape[0] / X.shape[0], 2) != train_size:\n",
    "        raise ValueError('Unexpected data quantity for training set!')\n",
    "    if np.around(X_test.shape[0] / X.shape[0], 2) != test_size:\n",
    "        raise ValueError('Unexpected data quantity for testing set!')\n",
    "    if np.around(X_val.shape[0] / X.shape[0], 2) != val_size:\n",
    "        raise ValueError('Unexpected data quantity for validation set!')\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea409aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify features and target variable\n",
    "\n",
    "feature_matrix = quake_data_df[['REGION_CODE', 'COUNTRY', 'YEAR', 'MONTH', 'LATITUDE', 'LONGITUDE', \n",
    "                                'FOCAL_DEPTH', 'EQ_PRIMARY', 'avg_pop_distance', 'HDI', 'sum_density', \n",
    "                                'dist_to_closest_hdi', 'unlogged_mag_val', 'avg_hdi_dist', 'FLAG_TSUNAMI']].copy()\n",
    "feature_matrix['FLAG_TSUNAMI'] = np.where(feature_matrix['FLAG_TSUNAMI'] == 'Yes', 1, 0)\n",
    "target_var = quake_data_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd557b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = basic_split(X=feature_matrix, y=target_var,\n",
    "                                                             train_size=0.6,\n",
    "                                                             test_size=0.2,\n",
    "                                                             val_size=0.2,\n",
    "                                                             random_state=112,\n",
    "                                                             stratify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm Stratification\n",
    "print(f'Val classes: \\n{y_val.value_counts()}')\n",
    "print(f'\\nTest classes: \\n{y_test.value_counts()}')\n",
    "print(f'\\nTrain classes: \\n{y_train.value_counts()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a7d45",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_ftrs = ['REGION_CODE', 'COUNTRY']\n",
    "std_ftrs = ['YEAR', 'MONTH', 'LATITUDE', 'LONGITUDE', 'sum_density', 'FOCAL_DEPTH', \n",
    "            'EQ_PRIMARY', 'avg_pop_distance', 'HDI', 'dist_to_closest_hdi', 'unlogged_mag_val', 'avg_hdi_dist']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "X_train_prep = np.hstack((clf.fit_transform(X_train), pd.DataFrame(X_train['FLAG_TSUNAMI'])))\n",
    "X_val_prep = np.hstack((clf.transform(X_val), pd.DataFrame(X_val['FLAG_TSUNAMI'])))\n",
    "X_test_prep = np.hstack((clf.transform(X_test), pd.DataFrame(X_test['FLAG_TSUNAMI'])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
